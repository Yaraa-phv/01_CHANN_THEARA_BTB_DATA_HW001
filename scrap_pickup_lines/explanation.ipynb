{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ed1538",
   "metadata": {},
   "source": [
    "<h1>Scrap Pickup Lines</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc436e7",
   "metadata": {},
   "source": [
    "First thing, is to \n",
    "- import BeautifulSoup and requests to our project and then \n",
    "- provide url link of targeted website\n",
    "- before .text, print(page) to see the response status\n",
    "- .text mean we want html code from the site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbcc95f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'https://www.womansday.com/relationships/dating-marriage/a41055149/best-pickup-lines/'\n",
    "page = requests.get(url).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63cc840",
   "metadata": {},
   "source": [
    "Then the soup here is to scrap page and parse to html format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d96cb7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page, 'html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bfa7b7",
   "metadata": {},
   "source": [
    "Let store an empty dict (info) first, then\n",
    "- find some of h2 tag that we want to find the title from website\n",
    "- loop to get only text, let print to see all titles\n",
    "- then find ul by using h2 mean \"find the first ul after/next h2\", if an h2 doesn't contain ul, just skip to another h2\n",
    "- next, store an empty list (lines), then find all li elements in ul\n",
    "- loop to get only content, then append/add to list\n",
    "- last is store both h2 and line with eah loop to info dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00d02fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {}\n",
    "\n",
    "# Find all title with h2 \n",
    "find_titles = soup.find_all('h2', title=True)\n",
    "\n",
    "for h2 in find_titles:\n",
    "    title = h2.text  \n",
    "    # print(title)\n",
    "\n",
    "    ul = h2.find_next('ul')  \n",
    "    \n",
    "    # If not found, just contiue to another h2\n",
    "    if not ul:\n",
    "        print(f\"There is a none of ul in h2 : {h2}\")\n",
    "        continue\n",
    "    \n",
    "    # find all li in ul\n",
    "    contents = ul.find_all('li')\n",
    "    lines = []\n",
    "    for li in contents:\n",
    "        pickup_lines = li.text\n",
    "        # print(\"pickup_lines\", pickup_lines)\n",
    "        lines.append(pickup_lines)\n",
    "    #     print(\"list of lines \", lines)\n",
    "    # print(\"finish loop...\", len(lines))\n",
    "    \n",
    "    # print(\"Add to dict ...\")\n",
    "    info[title] = lines\n",
    "    # print(info)\n",
    "    # print(f\"finish with {len(info)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3d86d",
   "metadata": {},
   "source": [
    "Lastly, let store the scraped data as a json file,\n",
    "- with json.dump() is about write the info into file with json format and pretty indention(indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9bf640",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickup_lines.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(info, file, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
