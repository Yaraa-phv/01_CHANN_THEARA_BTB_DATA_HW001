{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d66d20e9",
   "metadata": {},
   "source": [
    "<h1>Scrap Computer Products</h1>\n",
    "First thing with only scrapy project is all about running these commands,\n",
    "<ul>\n",
    "<li>scrapy startproject folder_name : make a folder with scrapy struture project, then cd to the project</li>\n",
    "<li>python -m venv venv : create a venv file</li>\n",
    "<li>.\\venv\\Scripts\\Activate.ps1 : to activate vene</li>\n",
    "<li>scrapy genspider spider_name url_to_scrap : create spider with default parse func</li>\n",
    "</ul>\n",
    "\n",
    "And the result as default is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15083b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class ScrapProductsSpider(scrapy.Spider):\n",
    "    name = \"scrap_products\"\n",
    "    allowed_domains = [\"www.goldonecomputer.com\"]\n",
    "    start_urls = [\"https://www.goldonecomputer.com/\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        links = response.xpath('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c0f602",
   "metadata": {},
   "source": [
    "<h3>First : Scrap all categories links</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbe7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "        # Scrap all links in categories\n",
    "        links = response.xpath('//*[@id=\"res-menu\"]/ul/li/a')\n",
    "        print(links)\n",
    "\n",
    "        for link in links:\n",
    "            category = link.css('::text').get()\n",
    "            url = link.xpath('@href').get()\n",
    "\n",
    "            # Validate the unwanted and none response\n",
    "            unwanted_titles = ['home', 'contact', 'about']\n",
    "\n",
    "            if not category or category.lower() in unwanted_titles or not url:\n",
    "                print(\"Skippy none title and url...\")\n",
    "                continue\n",
    "\n",
    "            # Response for categories.json \n",
    "            yield {\n",
    "                    'category' : category,\n",
    "                    'url' : url\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d258d9",
   "metadata": {},
   "source": [
    "<h3>Second : Follow Category Links and Scrap all product detail links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba0f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class ScrapProductsSpider(scrapy.Spider):\n",
    "    name = \"scrap_products\"\n",
    "    allowed_domains = [\"www.goldonecomputer.com\"]\n",
    "    start_urls = [\"https://www.goldonecomputer.com/\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Scrap all links in categories\n",
    "        links = response.xpath('//*[@id=\"res-menu\"]/ul/li/a')\n",
    "        print(links)\n",
    "\n",
    "        for link in links:\n",
    "            category = link.css('::text').get()\n",
    "            url = link.xpath('@href').get()\n",
    "\n",
    "            # Validate the unwanted and none response\n",
    "            unwanted_titles = ['home', 'contact', 'about']\n",
    "\n",
    "            if not category or category.lower() in unwanted_titles or not url:\n",
    "                print(\"Skippy none title and url...\")\n",
    "                continue\n",
    "\n",
    "            # Response for categories.json \n",
    "            # yield {\n",
    "            #         'category' : category,\n",
    "            #         'url' : url\n",
    "            #     }\n",
    "\n",
    "            yield scrapy.Request(\n",
    "                url = url,\n",
    "                callback = self.parse_categories,\n",
    "                meta = {'category' : category}\n",
    "            )\n",
    "\n",
    "    def parse_categories(self, response):\n",
    "        category = response.meta['category']\n",
    "        detail_links = response.xpath('//div[@id=\"content\"]//div[contains(@class, \"product-block product-thumb\")]//a')\n",
    "\n",
    "        products = {}\n",
    "        for link in detail_links:\n",
    "            product = link.css('::text').get()\n",
    "            product_link = link.xpath('@href').get()\n",
    "\n",
    "            if not product and product_link:\n",
    "                print(\"Skipping the none product and product link...\")\n",
    "                continue    \n",
    "\n",
    "            products[product] = product_link   \n",
    "\n",
    "        yield {\n",
    "                category : products\n",
    "            } \n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc888a4",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import json\n",
    "\n",
    "class ScrapProductLinksSpider(scrapy.Spider):\n",
    "    name = \"scrap_product_links\"\n",
    "    allowed_domains = [\"www.goldonecomputer.com\"]\n",
    "    start_urls = [\"https://www.goldonecomputer.com/\"]\n",
    "\n",
    "    def request_each_links(self):\n",
    "        with open('categories_links.json', 'r') as f:\n",
    "            urls = json.laod(f)\n",
    "        for item in urls:\n",
    "            yield scrapy.Request(url=item['url'], callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        detail_links = response.xpath('//div[@id=\"content\"]//div[contains(@class, \"product-block product-thumb\")]//a')\n",
    "\n",
    "        for link in detail_links:\n",
    "            product = link.css('::text').get()\n",
    "            product_link = link.xpath('@href').get()\n",
    "\n",
    "            if not product and product_link:\n",
    "                print(\"Skipping the none product and product link...\")\n",
    "                continue\n",
    "            \n",
    "            yield {\n",
    "                'product' : product,\n",
    "                'link' : product_link\n",
    "            }\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
