{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d66d20e9",
   "metadata": {},
   "source": [
    "<h1>Scrap Computer Products</h1>\n",
    "First thing with only scrapy project is all about running these commands,\n",
    "<ul>\n",
    "<li>scrapy startproject folder_name : make a folder with scrapy struture project, then cd to the project</li>\n",
    "<li>python -m venv venv : create a venv file</li>\n",
    "<li>.\\venv\\Scripts\\Activate.ps1 : to activate vene</li>\n",
    "<li>scrapy genspider spider_name url_to_scrap : create spider with default parse func</li>\n",
    "</ul>\n",
    "\n",
    "And the result as default is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15083b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class ScrapProductsSpider(scrapy.Spider):\n",
    "    name = \"scrap_products\"\n",
    "    allowed_domains = [\"www.goldonecomputer.com\"]\n",
    "    start_urls = [\"https://www.goldonecomputer.com/\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        links = response.xpath('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c0f602",
   "metadata": {},
   "source": [
    "<h3>First : Scrap all categories links</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbe7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "        # Scrap all links in categories\n",
    "        links = response.xpath('//*[@id=\"res-menu\"]/ul/li/a')\n",
    "        print(links)\n",
    "\n",
    "        for link in links:\n",
    "            category = link.css('::text').get()\n",
    "            url = link.xpath('@href').get()\n",
    "\n",
    "            # Validate the unwanted and none response\n",
    "            unwanted_titles = ['home', 'contact', 'about']\n",
    "\n",
    "            if not category or category.lower() in unwanted_titles or not url:\n",
    "                print(\"Skippy none title and url...\")\n",
    "                continue\n",
    "\n",
    "            # Response for categories.json \n",
    "            yield {\n",
    "                    'category' : category,\n",
    "                    'url' : url\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d258d9",
   "metadata": {},
   "source": [
    "<h3>Second : Follow Category Links and Scrap all product detail links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba0f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class ScrapProductsSpider(scrapy.Spider):\n",
    "    name = \"scrap_products\"\n",
    "    allowed_domains = [\"www.goldonecomputer.com\"]\n",
    "    start_urls = [\"https://www.goldonecomputer.com/\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Scrap all links in categories\n",
    "        links = response.xpath('//*[@id=\"res-menu\"]/ul/li/a')\n",
    "        print(links)\n",
    "\n",
    "        for link in links:\n",
    "            category = link.css('::text').get()\n",
    "            url = link.xpath('@href').get()\n",
    "\n",
    "            # Validate the unwanted and none response\n",
    "            unwanted_titles = ['home', 'contact', 'about']\n",
    "\n",
    "            if not category or category.lower() in unwanted_titles or not url:\n",
    "                print(\"Skippy none title and url...\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            yield scrapy.Request(\n",
    "                url = url,\n",
    "                callback = self.parse_categories,\n",
    "                meta = {'category' : category}\n",
    "            )\n",
    "\n",
    "    def parse_categories(self, response):\n",
    "        category = response.meta['category']\n",
    "        detail_links = response.xpath('//div[@id=\"content\"]//div[contains(@class, \"product-block product-thumb\")]//a')\n",
    "\n",
    "        products = {}\n",
    "        for link in detail_links:\n",
    "            product = link.css('::text').get()\n",
    "            product_link = link.xpath('@href').get()\n",
    "\n",
    "            if not product and product_link:\n",
    "                print(\"Skipping the none product and product link...\")\n",
    "                continue    \n",
    "\n",
    "            products[product] = product_link   \n",
    "\n",
    "        yield {\n",
    "                category : products\n",
    "            } \n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc888a4",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import json\n",
    "\n",
    "class ScrapProductLinksSpider(scrapy.Spider):\n",
    "    name = \"scrap_product_links\"\n",
    "    allowed_domains = [\"www.goldonecomputer.com\"]\n",
    "    start_urls = [\"https://www.goldonecomputer.com/\"]\n",
    "\n",
    "    def request_each_links(self):\n",
    "        with open('categories_links.json', 'r') as f:\n",
    "            urls = json.laod(f)\n",
    "        for item in urls:\n",
    "            yield scrapy.Request(url=item['url'], callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        detail_links = response.xpath('//div[@id=\"content\"]//div[contains(@class, \"product-block product-thumb\")]//a')\n",
    "\n",
    "        for link in detail_links:\n",
    "            product = link.css('::text').get()\n",
    "            product_link = link.xpath('@href').get()\n",
    "\n",
    "            if not product and product_link:\n",
    "                print(\"Skipping the none product and product link...\")\n",
    "                continue\n",
    "            \n",
    "            yield {\n",
    "                'product' : product,\n",
    "                'link' : product_link\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61316c30",
   "metadata": {},
   "source": [
    "<h3>The Last Updated : </h3>\n",
    "<ul>\n",
    "<li><b>def parse(self, response):</b> use for scrap all the category links, after get links loop it. Each link, use scrapy.Request to call the parse_category_page to work</li>\n",
    "<li><b>def parse_category_page(self, response):</b> use for scrap all the product links after goes to each category. Then loop for each link call the parse_product_detail to work. But for the pagination we handle with response.follow to follow next page</li>\n",
    "<li><b>def parse_product_detail(self, response):</b> use for scrap all detail data of the product from the gaven link. Use item = ScrapComProductsItem toâ€‹ build the appearence of the data with item.py and pipeline.py</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad1952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrap_com_products.items import ScrapComProductsItem\n",
    "\n",
    "\n",
    "class ScrapProductsSpider(scrapy.Spider):\n",
    "    name = \"scrap_products\"\n",
    "    allowed_domains = [\"www.goldonecomputer.com\"]\n",
    "    start_urls = [\"https://www.goldonecomputer.com/\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Get category links\n",
    "        links = response.xpath('//*[@id=\"res-menu\"]/ul/li/a')\n",
    "        # print(f'Found {len(links)} of total links')\n",
    "\n",
    "        for link in links:\n",
    "            category = link.xpath('string(.)').get(default='No Category').strip()\n",
    "            url = link.xpath('@href').get()\n",
    "\n",
    "            # Skip unwanted categories\n",
    "            unwanted = ['home', 'contact', 'about']\n",
    "            \n",
    "            if not category or not url or category.lower() in unwanted:\n",
    "                print(f\"Skipping none category or url : {category}\")\n",
    "                continue\n",
    "\n",
    "            # print(f\"=====> Processing category: {category}\")\n",
    "            \n",
    "            yield scrapy.Request(\n",
    "                url=url,\n",
    "                callback=self.parse_category_page,\n",
    "                meta={'category': category}\n",
    "            )\n",
    "\n",
    "    def parse_category_page(self, response):\n",
    "        category = response.meta['category']\n",
    "        \n",
    "        product_links = []\n",
    "        \n",
    "        detail_links = response.xpath('//div[contains(@class, \"product-thumb\")]//h4/a/@href').getall()\n",
    "        if detail_links:\n",
    "            product_links = detail_links\n",
    "            # print(f\"Found {len(detail_links)} product detail_links\")\n",
    "\n",
    "        for product_url in product_links:\n",
    "            yield scrapy.Request(\n",
    "                url=product_url,\n",
    "                callback=self.parse_product_detail,\n",
    "                meta={'category': category}\n",
    "            )\n",
    "\n",
    "        next_page = response.xpath('//ul[@class=\"pagination\"]//a[contains(text(), \">\")]/@href').get()\n",
    "        if next_page:\n",
    "            yield response.follow(\n",
    "                next_page,\n",
    "                callback=self.parse_category_page,\n",
    "                meta={'category': category}\n",
    "            )\n",
    "\n",
    "    def parse_product_detail(self, response):\n",
    "        category = response.meta['category']\n",
    "        title = response.xpath('//div[@id=\"content\"]//h3/text()').get(default='No Title').strip()\n",
    "        brand = response.xpath('//ul[@class=\"list-unstyled\"]/li[span[text()=\"Brand:\"]]/a/text()').get(default='No Brand').strip()\n",
    "        code = response.xpath('//ul[@class=\"list-unstyled\"]/li[span[text()=\"Product Code:\"]]/text()').get(default='No Code').replace('Product Code:', '').strip()\n",
    "        price = response.xpath('//ul[@class=\"list-unstyled price\"]/li/h3/text()').get(default='No Price').strip()\n",
    "        image_link = response.xpath('//a[@class=\"thumbnail\"]/@href').get()\n",
    "        if not image_link:\n",
    "            image_link = response.xpath('//*[@id=\"content\"]/div[1]/div[1]/div/div/div//img/@src').get()\n",
    "        review = response.xpath('//a[contains(@class, \"review-count\")]/text()').get(default='No Review').strip()\n",
    "            \n",
    "             # products = {}\n",
    "        # for url in product_links:\n",
    "        #     products = {\n",
    "        #         'title': title,\n",
    "        #         'brand': brand,\n",
    "        #         'code': code,\n",
    "        #         'price': price,\n",
    "        #         'image_link': image_link,\n",
    "        #         'review': review,\n",
    "        #         'url' : url\n",
    "        #     }\n",
    "        \n",
    "        # yield{\n",
    "        #     category : products\n",
    "        # }\n",
    "            \n",
    "            \n",
    "        # Create the item\n",
    "        item = ScrapComProductsItem(\n",
    "            category=category,\n",
    "            product_data={\n",
    "                'category' : category,\n",
    "                'title': title,\n",
    "                'brand': brand,\n",
    "                'code': code,\n",
    "                'price': price,\n",
    "                'image_link': image_link,\n",
    "                'review': review,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        yield item"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
